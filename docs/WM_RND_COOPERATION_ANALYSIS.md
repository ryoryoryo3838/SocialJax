# 協調的振る舞いの創発における課題と改善案 (WM-RND)

本ドキュメントでは、現在の Model-Based Symmetry and Novelty Exploration (WM-RND) 実装において、なぜ協調的な振る舞い（Emphasized Policy Emergence）が十分に生まれない可能性があるのか、その構造的な問題点と改善案を分析します。

---

## 1. 現状のコードと構造的課題

### 1.1 Symmetry（強調方策）の適用範囲が限定的である
- **現状**: Symmetry（自己投影）のロジックは **「イマジネーション（世界モデル内での学習）」** にのみ適用されています。
- **問題**: 実環境での行動選択（PPOの学習）は、あくまで **「自分個人の報酬」** を最大化するように行われています。
  - イマジネーションで「協力したら良い結果になる」と学習しても、実環境の探索（Rollout）で「自分がサボって他人が働いたときが一番楽（高報酬）」という経験を積んでしまうと、PPOの更新でその「利己的な成功体験」が優先されがちです。
  - イマジネーションの回数（`IMAGINATION_STEPS`）や影響力（`IMAGINATION_COEF`）が、実体験のインパクトに比べて小さい可能性があります。

### 1.2 世界モデルの予測精度と報酬構造のギャップ
- **現状**: 世界モデルは `next_latent` と `rewards` を予測しますが、特に `rewards` の予測が重要です。
- **課題**: 協調行動（例: 川の掃除）は「行動（掃除）してから報酬（リンゴ出現）までの時間ラグ」が長く、かつ因果関係が間接的（掃除した人 $\neq$ 食べる人）です。
  - 単純なMLP（全結合層）の世界モデルでは、この「誰かの掃除が巡り巡って全員のリンゴになる」という複雑な因果関係を、しかも **少ないステップ数（`IMAGINATION_STEPS`）** で学習・予測するのは非常に困難です。
  - 結果、イマジネーションの中で「掃除しても何も起きない（無駄骨）」と誤学習し、協調行動が消去される恐れがあります。

### 1.3 RND（新規性探索）と協調のジレンマ
- **現状**: RNDは「見たことがない状態」に報酬を与えます。
- **問題**: 「誰もいない場所に行く」や「ランダムに動き回る」といった個人的な探索行動の方が、手っ取り早く新規性報酬（Min-Max正規化で常に高い報酬）を得られやすいです。
  - 一方、「定位置で掃除し続ける」ような協調行動は、視覚的な変化（新規性）が乏しく、RNDによる加点は低くなりがちです。
  - **「協調行動＝退屈な作業」** という構造があるため、内発的動機づけ（好奇心）が協調を阻害する要因になり得ます。

---

## 2. 改善案と推奨アクション

### 2.1 Symmetryの強化：実環境報酬への介入 (Counterfactual Baseline)
- **提案**: 実環境の報酬（External Reward）自体にも、Symmetryの考え方を導入する。
- **具体策**: 「もし全員が自分と同じ行動をしていたら得られたであろう報酬（Counterfactual Reward）」を推定し、それを報酬の一部として加算する。
  - これにより、イマジネーションだけでなく、実体験のフィードバック自体が「普遍化可能性」を帯びるようになります。

### 2.2 世界モデルの長期予測強化 (Transformer / RNN)
- **提案**: 世界モデルの構造を MLP から **Transformer** や **RNN (GRU/LSTM)** ベースに変更する。
- **メリット**: 「過去の掃除行動」が「未来のリンゴ出現」にどう影響するかという長期的な依存関係（Credit Assignment）を捉えやすくなります。
- **実装イメージ**: `WorldModel` クラスに `nn.GRUCell` や `nn.SelfAttention` を組み込み、時系列情報を処理・記憶できるようにする。

### 2.3 協調特化型RND (Team Novelty)
- **提案**: 「個人の新規性」だけでなく、「チーム全体の配置や状態の新規性」を評価する。
- **具体策**:
  - 全エージェントの観測を結合したものをRNDに入力する（Centralized RND）。
  - 「全員が散らばっている状態」よりも「特定のフォーメーション（掃除役と採取役）を組んでいる状態」が未体験であれば、そこに高い報酬を与える。
  - これにより、「協力プレイ」自体を新しい体験として探索させることができます。

### 2.4 イマジネーションの質の向上 (DreamerV3的アプローチ)
- **提案**: イマジネーションの回数をやすだけでなく、**「成功したイマジネーション（高報酬なエピソード）」を優先的に学習に使う（Prioritized Experience Replay）**。
- **効果**: 偶然にでも「協力してうまくいった」シミュレーション結果を重点的に反復学習させることで、稀な協調体験の学習効率を高めます。

---

## 3. 次に取り組むべきこと（優先度順）

1.  **世界モデルの系列モデル化**: 行動と結果の時間ラグを埋めるために、RNN/Transformer化を検討する。
2.  **Symmetry係数（`SYMMETRY_RATIO`）の調整実験**: 現状の `1.0` (完全強調) から徐々に下げるスケジュールや、逆に強調し続ける設定の効果を検証する。
3.  **実報酬への介入実装**: PPOの `rewards` にも Symmetry ロジック（平均報酬など）を混ぜる実装を試す。
