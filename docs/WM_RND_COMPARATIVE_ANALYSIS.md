# 自己投影型MARLによる協調行動の創発：文献比較と考察

本ドキュメントでは、**「独立した観測・報酬・行動を持ちながら、モデル内での自己投影（Self-Projection）を通じて協調を創発させる」** という構想について、既存のMARL文献と比較しつつ、その独自性と実現のための実装指針を考察します。

## 1. コンセプトの核心：定言命法と超合理性

あなたの意図するアプローチは、ゲーム理論や哲学における以下の概念の実装と捉えられます。

*   **カントの定言命法 (Categorical Imperative)**: 「自分の行動が普遍的な法則となるように行為せよ」
*   **超合理性 (Superrationality)**: 「自分と相手が同じ論理的思考能力を持つならば、相手も自分と同じ結論（行動）に達するはずだ」という仮定に基づき、双方にとって最適な協調解を選択すること。

これは、従来のMARLが陥りやすい「相手を環境の一部（外乱）として扱う」アプローチとは一線を画し、**「相手を自己の鏡像（Mirror Image）として扱う」** ことで社会的ジレンマを突破しようとする野心的な試みです。

---

## 2. 関連文献との比較分析

提示された文献とあなたのコード（WM-RND）のスタンスを比較します。

| 項目 | **CoDreamer** (Arxiv:2406.13600) | **DIMA** (Arxiv:2505.20922) | **MAMBA** (Arxiv:2205.15023) | **WM-RND (本手法)** |
| :--- | :--- | :--- | :--- | :--- |
| **協調の手段** | **通信 (Communication)** | **高精度モデリング** | **集中学習 (CTDE)** | **自己投影 (Self-Projection)** |
| **世界モデル** | エージェント間でメッセージ交換 | 拡散モデルで複雑な結合分布を表現 | 通信で情報を補完 | **「他者＝自分」と仮定して単純化** |
| **他者の扱い** | 情報をくれるパートナー | 予測困難な確率変数 | 訓練時の情報源 | **自己の複製 (Cognitive Clone)** |
| **前提条件** | 通信回線の存在 | 高い計算リソース | 訓練時の全情報アクセス | **エージェント間の認知的均質性** |

### 各手法との決定的な違い

1.  **対 CoDreamer (通信なしの優位性)**
    *   CoDreamerは「話し合って解決」しますが、通信コストやレイテンシの問題があります。
    *   本手法は「**察して解決**（暗黙の協調）」を目指します。全員が同じ「自己投影」という思考プロトコルを持つことで、言葉を交わさずとも阿吽の呼吸で動くことを狙います。これは生物（群知能）に近いアプローチです。

2.  **対 DIMA (複雑性の回避)**
    *   DIMAは他者の動きを「複雑な分布」として学習しようとしますが、エージェント数が増えると学習は困難になります。
    *   本手法は「他者は自分のように動くはずだ」という**強いバイアス（仮定）** を置くことで、世界モデルの学習コストを劇的に下げ（$N$人分の行動空間 $\to$ 1人分）、かつ協調解への収束を早めることができます。

3.  **対 MAMBA (独立性の保持)**
    *   MAMBAは訓練時に情報を集約しますが、本手法は訓練時も独立（Decentralized Training）であり、より現実的な設定です。

---

## 3. 「行動の漸近」を実現するための実装指針

「パラメータ共有はしない」という現実的な制約の下で、**「モデル内での学習結果を現実でも振る舞うことで、全員が無意識に漸近していく」** 状態を作るには、単なる自己投影（Symmetry）以上の**「認知的同調メカニズム」**が必要です。

以下の3つのステップで実装を強化することを推奨します。

### ステップ1: 自己投影イマジネーション (Self-Projection Imagination)
*現状の実装の維持・強化*

世界モデル内で、他者の行動入力として自分のPolicyの出力を使うこと自体は非常に有効です。
*   **効果**: 今まで「他者がランダムに動くから自分も勝手にする」と学習していたのが、「自分が協力すれば（投影された）他者も協力してくれる」世界をシミュレートできるようになり、協調行動の価値（Value）が高く見積もられます。

### ステップ2: 行動一致性損失 (Behavioral Consistency/Cloning Loss)
*新規追加の推奨*

パラメータ共有なしで他者と行動を似せるには、**「現実の他者の行動」を教師データとして自分を微修正する**プロセスが有効です。

*   **概念**: 「私が世界モデルでシミュレートした他者（＝私）」と「現実の他者」の動きが食い違っている場合、それは**私の「自己投影」がまだ現実と同期していない**ことを意味します。
*   **実装**:
    現実の Rollout で得られた他エージェントの行動 $a_{other}$ と、自分の現在の Policy $\pi_{self}(s)$ が出す行動分布との間の距離（KLダイバージェンス等）を最小化する損失項を加えます。
    $$ L_{consistency} = \text{KL}(\pi_{self}(s) || a_{other}) $$
    *   **注釈**: これは「他者の真似をする」ことそのものが目的ではなく、**「他者も自分と同じ論理で動いているはずだ」という仮説を現実に強制適合させる**（Belief Matching）処理です。これにより、学習が進むにつれて全員のPolicyが数学的に近づいていきます。

### ステップ3: 楽観的価値更新 (Optimistic Value Update)

現実がまだ協調的でなくても、脳内（モデル）での協調体験を信じさせる工夫です。

*   **現状**: PPOは「現実の報酬」に基づいて更新されます。現実で他者が裏切れば、モデル内の協調プランは否定されます。
*   **改善**: Value Functionのターゲット計算において、現実の報酬 $R_{real}$ と、自己投影シミュレーションでの報酬 $R_{imagined}$ を混合します。
    $$ V_{target} = (1 - \lambda) V(R_{real}) + \lambda V(R_{imagined}) $$
*   **効果**: 「現実はまだ厳しいが、理想（協調）の方が得だ」という信念を持ち続けさせます。全員がこの「楽観的な信念」を持ち続けることで、現実側の行動が徐々に理想（協調）に引き寄せられていきます（自己成就的予言）。

---

## 4. 結論

あなたのコードのアプローチは、既存文献の主流（通信や高機能モデル）とは異なる**「認知バイアスの工学的利用」**という独自のアプローチをとっており、非常に興味深いです。

**「パラメータ共有なし」で「行動の漸近」を達成する鍵**は、以下のループを回すことです：
1.  **脳内**: 「他者は自分だ」と仮定して、協調が最適解であると学習する。
2.  **行動**: その学習結果に基づいて、現実で協調的な行動をとる。
3.  **観察**: 現実の他者の行動を見て、自分の行動指針とのズレ（Consistensy Loss）を修正する。
    *   全員がこれを行えば、全員が「協調的な行動」へと引き寄せられ（Attractor）、結果として**パラメータは異なれど、振る舞い（Phenotype）は同一化**します。

この方向性で WM-RND を拡張・調整していくのが、あなたの研究意図に最も適していると考えられます。
