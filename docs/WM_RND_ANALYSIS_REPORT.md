# WM-RND (World Model with Emphasized Policy Emergence) Analysis Report

## 1. アルゴリズム概要
本実装は、**Model-Based Multi-Agent Reinforcement Learning (MB-MARL)** の一種であり、特に「カント的定言命法」を模した**Symmetry（対称性・自己投影）**の仮定を導入することで、社会的ジレンマ環境における協調行動（強調方策）の創発を目指しています。

---

## 2. 詳細仕様分析

### 2.1 世界モデル (World Model)
- **入力**: エージェント個人の観測 ($o_t^i$) + **全エージェントの行動** ($a_t^{1...N}$)
- **出力**: 次の潜在状態 ($z_{t+1}^i$) + **全エージェントの報酬** ($r_t^{1...N}$)
- **特徴**:
  - 各エージェントは「自分が見えている世界」が「全員の行動」によってどう変化し、その結果「全員がどう報われるか」を学習します。
  - 実装クラス: `WorldModel` (`wm_rnd_networks.py`)

### 2.2 強調方策創発メカニズム (Symmetry / Self-Projection)
- **概念**: 学習中のイマジネーション（脳内シミュレーション）において、「もし他の全エージェントが自分と全く同じ方策（判断基準）で行動したらどうなるか？」を問います。
- **実装**:
  - `wm_rnd.py` 内の `imag_rollout` 関数にて、自身の方策出力 `logits` を `jnp.repeat` で全エージェント分に複製。
  - その複製された行動を用いて世界モデルを推論させます。
  - 予測された**全エージェントの平均報酬**を最大化するように自身の方策を更新します。
- **効果**: 「自分だけが得をする行動」ではなく、「全員が真似しても破綻せず、かつ全体利益が高まる行動（普遍化可能な行動）」が学習されます。

### 2.3 新規性探索 (RND: Random Network Distillation)
- **仕様**: 全エージェントで共有された Target/Predictor ネットワークを使用。
- **目的**: チーム全体として未知の状態（観測）を探索することを奨励。
- **実装**: 共有されているため、エージェントAが探索済みの場所は、エージェントBにとっても「既知」として扱われます（報酬が出ない）。

---

## 3. 課題と問題点

### 3.1 世界モデルの入力と部分観測性の乖離
- **現状**: 世界モデルは「全エージェントの行動」を入力として受け取ります。
- **問題**: 部分観測環境（POMDP）において、エージェントは他者の正確な行動を知り得ない場合が多いです。学習時（Centralized Training）は問題ありませんが、モデル自体の汎用性や、実行時にモデルを使いたい場合（プランニングなど）に制約となります。
- **リスク**: 「他者の行動」に過度に依存したモデルになり、他者の行動が見えない状況での予測精度が落ちる可能性があります。

### 3.2 共有RNDによる「役割分担」の強制化
- **現状**: RNDネットワークが全エージェントで共有されています。
- **問題**: エージェントAがある部屋を探索すると、エージェントBはその部屋に入っても新規性報酬を得られません。
- **影響**: 効率的な探索（手分けして探す）には寄与しますが、各エージェントが個別に環境を理解する必要があるタスク（例：全員がゴールに到達する必要がある迷路など）では、学習を阻害する可能性があります。

### 3.3 Symmetry（自己投影）による多様性の欠如
- **現状**: 「全員が自分と同じ」という強い仮定を置いています。
- **問題**: サッカーやOvercookedのような「異質な役割分担（攻めと守り、切ると焼く）」が必要なタスクでは、この仮定が足枷になります。「自分が守るなら、全員が守る」というシミュレーションをしてしまい、「自分が守るなら、他は攻めるべき」という補完的な関係を学習しにくい構造です。

### 3.4 計算コストとスケーラビリティ
- **現状**: `imag_steps` 分のロールアウトをPPOの更新ごとに毎回行うため、計算コストが高いです。
- **課題**: エージェント数が増えると `joint_actions` の次元が増大し、世界モデルの学習が難しくなります。

---

## 4. 改善案と推奨アクション

### 4.1 RNDの個別化（またはハイブリッド化）
- **提案**: 各エージェントに固有の RND Predictor を持たせる。
- **メリット**: 各個体がそれぞれの視点で環境を理解・探索できるようになり、個人の経験知が蓄積されます。
- **実装案**: `wm_rnd.py` で `rnd_predictor` を `jax.vmap` でエージェント数分作成し、個別に更新する。

### 4.2 世界モデルの入力の再考
- **提案**: 他者の行動を直接入力せず、「他者の行動の影響を含んだ環境変化」として捉えるか、あるいは「他者の行動を予測する」モジュールを追加する。
- **簡易案**: 現状のままでも「Symmetry仮定」の下では「他者の行動＝自分の行動」として生成されるため矛盾は少ないですが、より一般的なMARLにするなら、入力は `(obs, my_action)` のみにし、他者の影響は確率的な遷移として扱う（Stochastic World Model）方が堅牢です。

### 4.3 Symmetry係数の導入と減衰
- **提案**: イマジネーション時の報酬計算において、`全報酬の平均` だけでなく `自分の報酬` も混ぜ合わせる。
- **式**: $R_{optimization} = (1 - \alpha) \cdot R_{self} + \alpha \cdot R_{mean}$
- **運用**: 学習初期は $\alpha=1.0$ で協調行動を強制し、後半にかけて $\alpha$ を下げていくことで、協調をベースにしつつも個別の役割分担（スペシャライゼーション）を許容できるようにする。

### 4.4 コード実装上の修正案 (Configuration)
`config` ファイルで以下のパラメータを制御できるように改修することを推奨します。
- `RND_SHARED`: RNDを共有するか個別にするかを選択可能にする。
- `SYMMETRY_RATIO`: 自己投影報酬の割合を調整可能にする。
