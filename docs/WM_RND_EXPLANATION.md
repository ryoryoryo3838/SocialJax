# WM-RND (World Model + Random Network Distillation) アルゴリズムガイド

このドキュメントでは、`components/algorithms/wm_rnd.py` に実装されている `wm_rnd` アルゴリズムの挙動と、その設定パラメータの調整方法について解説します。

## アルゴリズムの概要

WB-RND (WM-RND) アルゴリズムは、マルチエージェントPPO (MAPPO/IPPO) をベースにしつつ、標準的なモデルフリー強化学習を以下の2つの要素で拡張したものです。

1.  **RND (Random Network Distillation)**: 内部動機づけ（探索）のため。
2.  **世界モデル (World Model) と 想像 (Imagination)**: 協調的な計画と長期的な視野での最適化のため。

これは、Harvest（収穫）やCleanUp（清掃）のような複雑なマルチエージェント調整問題を解決するために設計されており、エージェントが新しい状態を探索することや、行動の結果を「想像」して協調的な戦略を見つけることを促します。

---

## 詳細な挙動

### 1. RND (Random Network Distillation) による探索
**目的:** エージェントが見慣れない状態を訪れるように促す。

*   **メカニズム:** エージェントは2つのネットワークを保持します。
    *   **ターゲットネットワーク (Target Network):** 固定されたランダムなネットワーク。
    *   **予測ネットワーク (Predictor Network):** ターゲットネットワークの出力を予測するように学習されるネットワーク。
*   **内部報酬 (Intrinsic Reward):** 予測ネットワークとターゲットネットワークの出力間の誤差 (MSE) を「内部報酬」として使用します。
    *   誤差が大きい $\rightarrow$ 新しい状態（まだ学習していない状態） $\rightarrow$ 報酬が高い。
    *   誤差が小さい $\rightarrow$ 見慣れた状態 $\rightarrow$ 報酬が低い。
*   **適用:** この内部報酬は、PPOの更新前に環境からの外部報酬に加算されます。
    $$Reward_{total} = Reward_{ext} + \text{INTRINSIC\_COEF} \times Reward_{int}$$

### 2. 世界モデル (World Model) と 想像 (Imagination)
**目的:** 他者が協調すると仮定した場合に、将来の報酬を最大化するように行動することを促す。

*   **世界モデル:** 以下を予測するように学習されるモデルです。
    *   次の潜在状態 (Latent State)。
    *   *全エージェント*の報酬。
*   **想像の展開 (Imagination Rollout):** PPOの学習中に、エージェントは世界モデルを使って `IMAGINATION_STEPS` 分の未来を「想像」します。
*   **「自己投影」による対称性 (Project Self / Symmetry):** 重要な点として、この想像の中でエージェントは「**他のすべてのエージェントが自分と全く同じように行動する**」と仮定します。
    *   自分のポリシー（方策）を他のすべてのエージェントのスロットに複製して適用します。
    *   実質的に、「もし全員が今の自分と同じ戦略をとったら、グループ全体の結果はどうなるか？」をシミュレーションします。
*   **補助損失 (Auxiliary Loss):** ポリシーは、この想像された軌道の「平均報酬」を最大化するように更新されます。これにより、全員が採用した場合に全体にとって有益となる戦略（協調）へとポリシーが誘導されます。

---

## 設定ガイド

設定は `scripts/config/algorithm/wm_rnd.yaml` で定義されています。以下は主要なパラメータの解説です。

### コア PPO パラメータ
| パラメータ | 説明 | 一般的な値 |
| :--- | :--- | :--- |
| `LR` | PPO（Actor-Criticネットワーク）の学習率。 | `1e-4` ～ `5e-4` |
| `GAMMA` | 将来の報酬に対する割引率。 | `0.99` ～ `0.999` |
| `ENT_COEF` | エントロピー係数。高い値ほどランダムな行動（探索）を強制し、早期収束を防ぎます。 | `0.01` ～ `0.05` |

### RND (探索) パラメータ
| パラメータ | 説明 | 調整のアドバイス |
| :--- | :--- | :--- |
| `INTRINSIC_COEF`| 実際の報酬に加算される内部報酬の重み。 | エージェントが何もせず、報酬も得られていない場合は**上げる**。タスクを無視して「うろつく」だけの行動が見られる場合は**下げる**。 |
| `RND_LR` | RND予測ネットワークの学習率。 | 通常、ポリシーの学習率と同じか少し高めに設定。 |

### 世界モデル (協調) パラメータ
| パラメータ | 説明 | 調整のアドバイス |
| :--- | :--- | :--- |
| `WM_LR` | 世界モデルの学習率。 | 環境のダイナミクスを素早く学習できる程度に高く設定する。 |
| `IMAGINATION_STEPS` | エージェントがどれくらい先の未来まで「想像」するか。 | 長期的な計画（例：農業）が必要なタスクでは**増やす**。反応的なタスクでは**減らす**。値が大きいと計算コストが増加する。 |
| `IMAGINATION_COEF` | 想像による損失がポリシー更新に与える影響の大きさ。 | エージェントに、モデルが見つけた「協調的な最適解」を優先させたい場合は**上げる**。モデルが不正確であったり学習が不安定な場合は**下げる**。 |
| `IMAGINATION_EPOCHS` | *注: 現在のコード実装では使用されていないようです。* | N/A |

---

## ソーシャルジレンマ（例：Harvest）に向けた調整戦略

1.  **エージェントが動かない / 資源を食べない？**
    *   報酬が疎（sparse）な問題です。
    *   **対策:** `INTRINSIC_COEF` を上げます（例: `0.1` -> `0.5`）。これにより、「新しいものを見る」こと自体が報酬となります。

2.  **エージェントは食べるが、資源を枯渇させてしまう（コモンズの悲劇）？**
    *   エージェントが貪欲で近視眼的になっています。
    *   **対策:** `IMAGINATION_COEF` を上げ、`IMAGINATION_STEPS` が十分な長さ（例: 32 や 64）であることを確認します。これにより、「今すべて食べてしまい、他のみんなもそうしたら（対称性）、後で全員飢える」ということを考慮させます。

3.  **学習が不安定（報酬が振動する）？**
    *   世界モデルが幻覚を見ている（不正確な予測をしている）ため、ポリシーを誤った方向に導いている可能性があります。
    *   **対策:** `IMAGINATION_COEF` を下げるか、世界モデルの学習を改善（`WM_LR` を上げる、またはモデルサイズを大きくする）します。あるいは、ポリシーの `LR` を下げてみます。

4.  **エージェントが局所解（Local Optima）にはまっている？**
    *   **対策:** `ENT_COEF` を上げて、よりランダムな試行錯誤を強制します。
